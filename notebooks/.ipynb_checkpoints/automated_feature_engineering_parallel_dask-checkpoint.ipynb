{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and approach:\n",
    "Data is from Kaggle competiotion [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk). \n",
    "\n",
    "I implement an automated feature engineering approach with an open-source library [Featuretools](https://www.featuretools.com/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/home_credit_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    \"\"\"Convert pandas data types for memory reduction.\"\"\"\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train = pd.read_csv('../data/application_train.csv', sep=',')\n",
    "app_test = pd.read_csv('../data/application_test.csv')\n",
    "bureau = pd.read_csv('../data/bureau.csv')\n",
    "bureau_balance = pd.read_csv('../data/bureau_balance.csv')\n",
    "cash = pd.read_csv('../data/POS_CASH_balance.csv')\n",
    "credit = pd.read_csv('../data/credit_card_balance.csv')\n",
    "previous = pd.read_csv('../data/previous_application.csv')\n",
    "installments = pd.read_csv('../data/installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.name = 'app_train'\n",
    "app_test.name = 'app_test'\n",
    "bureau.name = 'bureau'\n",
    "bureau_balance.name = 'bureau_balance'\n",
    "cash.name = 'cash'\n",
    "credit.name = 'credit'\n",
    "previous.name = 'previous'\n",
    "installments.name = 'installments'\n",
    "\n",
    "datasets_list = [app_train, app_test, bureau, bureau_balance, cash, credit, previous, installments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the anomalous values\n",
    "for ds in datasets_list:\n",
    "    ds.replace({365243: np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_train\t - \t307510 rows\n",
      "app_test\t - \t48744 rows\n",
      "bureau\t - \t1716420 rows\n",
      "bureau_balance\t - \t27299925 rows\n",
      "cash\t - \t10001358 rows\n",
      "credit\t - \t3840312 rows\n",
      "previous\t - \t1670214 rows\n",
      "installments\t - \t13605401 rows\n"
     ]
    }
   ],
   "source": [
    "# Numbers of rows:\n",
    "for ds in datasets_list:\n",
    "    print('{}\\t - \\t{} rows'.format(ds.name , ds.iloc[:, 0].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join train and test set to make sure, that the same feature are created for each set. \n",
    "app_test['TARGET'] = np.nan\n",
    "app = app_train.append(app_test, ignore_index=True)\n",
    "\n",
    "# Need 'SK_ID_CURR in each table (for make partitioning possible )\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], how='left', on = 'SK_ID_BUREAU')\n",
    "\n",
    "\n",
    "# Convert types to reduce memory usage\n",
    "app = convert_types(app)\n",
    "bureau = convert_types(bureau)\n",
    "bureau_balance = convert_types(bureau_balance)\n",
    "cash = convert_types(cash)\n",
    "credit = convert_types(credit)\n",
    "previous = convert_types(previous)\n",
    "installments = convert_types(installments)\n",
    "\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_sum = round(np.sum([app_train.memory_usage().sum()/ 1e9 for x in datasets_list]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of data: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "print('Total size of data: {} GB'.format(memory_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data (partitioning) for parallel computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_path = 'D:/DYSK/home_credit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition, external_path):\n",
    "    \"\"\"Creates and saves a dataset with only the users in `user_list`.\"\"\"\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = external_path + '/data/partitions/p%d' % (partition + 1)\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        # Subset based on user list\n",
    "        app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "        bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "\n",
    "        # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "        bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "        installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # Save data to the directory\n",
    "        app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "        bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "        bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "        cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "        credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "        previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "        installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "        if partition % 10 == 0:\n",
    "            print('Saved all files in partition {} to {}.'.format(partition + 1, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create id_list of indecies\n",
    "chunk_size = app.shape[0] // 103\n",
    "id_list = [ list(app.iloc[i: i + chunk_size ].index) for i in range(0, app.shape[0], chunk_size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all files in partition 81 to D:/DYSK/home_credit/data/partitions/p81.\n",
      "Saved all files in partition 91 to D:/DYSK/home_credit/data/partitions/p91.\n",
      "Saved all files in partition 101 to D:/DYSK/home_credit/data/partitions/p101.\n",
      "Partitioning took 916 sec.\n"
     ]
    }
   ],
   "source": [
    "#### Create partitions\n",
    "start = time.time()\n",
    "\n",
    "for i, ids in enumerate(id_list):\n",
    "    create_partition(user_list= ids, partition= i, external_path= external_path)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Partitioning took {} sec.'.format(round(end - start), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features definitions previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_defs = ft.load_features('../input/features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_defs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create EntitySet from Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path.\n",
    "       Returns a dictionary with the entityset and the number used for saving the feature matrix.\"\"\"\n",
    "    \n",
    "    external_path = 'D:/DYSK/home_credit'\n",
    "    \n",
    "    partition_num = int(path[18:])\n",
    "    \n",
    "    # Read in data\n",
    "    app = pd.read_csv('%s/app.csv' % (external_path + path))\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % (external_path + path))\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % (external_path + path))\n",
    "    previous = pd.read_csv('%s/previous.csv' % (external_path + path))\n",
    "    credit = pd.read_csv('%s/credit.csv' % (external_path + path))\n",
    "    installments = pd.read_csv('%s/installments.csv' % (external_path + path))\n",
    "    cash = pd.read_csv('%s/cash.csv' % (external_path + path))\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return ({'es': es, 'num': partition_num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/partitions/p1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_dict = entityset_from_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 3458, Columns: 123]\n",
       "    bureau [Rows: 3458, Columns: 18]\n",
       "    previous [Rows: 3458, Columns: 38]\n",
       "    bureau_balance [Rows: 3458, Columns: 5]\n",
       "    cash [Rows: 3458, Columns: 9]\n",
       "    installments [Rows: 3458, Columns: 9]\n",
       "    credit [Rows: 3458, Columns: 24]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_dict['es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create Feature Matrix from EntitySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es_dict, feature_defs, return_fm = False):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature definitions. \n",
    "    Saves feature matrix based on partition.\"\"\" \n",
    "    \n",
    "    external_path = 'D:/DYSK/home_credit'\n",
    "    \n",
    "    \n",
    "    # Extract the entityset\n",
    "    es = es_dict['es']\n",
    "    \n",
    "    # Calculate the feature matrix and save\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_defs, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = es['app'].df.shape[0])\n",
    "    \n",
    "    feature_matrix.to_csv( external_path + '/data/fm/p%d_fm.csv' % es_dict['num'], index = True)\n",
    "    \n",
    "    if return_fm:\n",
    "        return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation took 24 sec.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "fm1 = feature_matrix_from_entityset(es_dict=es_dict, feature_defs= feature_defs, return_fm=True)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Computation took {} sec.'.format(round(end - start), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 1820)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the system memory for a full run of Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2416"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  gc\n",
    "\n",
    "# Free up all system memory \n",
    "gc.enable()\n",
    "del app, bureau, bureau_balance, previous, credit, cash, installments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  dask.bag as db\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Use all 8 cores\n",
    "client = Client(processes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:59491': 1,\n",
       " 'tcp://127.0.0.1:59492': 1,\n",
       " 'tcp://127.0.0.1:59495': 1,\n",
       " 'tcp://127.0.0.1:59504': 1}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ncores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [external_path + '/data/partitions/p%d' %  i for i in range(1, 105)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag object\n",
    "b = db.from_sequence(paths)\n",
    "\n",
    "# Map entityset function\n",
    "b = b.map(entityset_from_partition)\n",
    "\n",
    "# Map feature matrix function\n",
    "b = b.map(feature_matrix_from_entityset, feature_defs = feature_defs)\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "b.compute()\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Computation took {} sec.'.format(round(end - start), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
